{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Clustering and Topic Modeling.ipynb”",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diwu437/diwu-github.io/blob/master/Document_Clustering_and_Topic_Modeling_ipynb%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3LTq59E8UK",
        "colab_type": "text"
      },
      "source": [
        "# Document Clustering and Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS-ob0Q8E8UM",
        "colab_type": "text"
      },
      "source": [
        "In this project, I used unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE7Qr-ccE8UN",
        "colab_type": "text"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S0waFW9E8UO",
        "colab_type": "text"
      },
      "source": [
        "* [Part 1: Load Data](#Part-1:-Load-Data)\n",
        "* [Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)\n",
        "* [Part 3: TF-IDF](#Part-3:-TF-IDF)\n",
        "* [Part 4: K-means clustering](#Part-4:-K-means-clustering)\n",
        "* [Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nXGAelMjJFq",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eT1n7oijJ8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N2bb9S6jKGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/open?id=192JMR7SIqoa14vrs7Z9BXO3iK89pimJL\n",
        "file = drive.CreateFile({'id':'192JMR7SIqoa14vrs7Z9BXO3iK89pimJL'}) # replace the id with id of file\n",
        "file.GetContentFile('data.tsv')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc9DK62BE8UP",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdBV8gGE8UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "import gensim\n",
        "# REGULAR EXPRESSION\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIj2Z8T70FZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data into dataframe\n",
        "df = pd.read_csv('data.tsv', sep='\\t', header=0, error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVIWPym-8MnJ",
        "colab_type": "code",
        "outputId": "c3932c71-1712-4492-d7e0-30a943b30345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>3653882</td>\n",
              "      <td>R3O9SGZBVQBV76</td>\n",
              "      <td>B00FALQ1ZC</td>\n",
              "      <td>937001370</td>\n",
              "      <td>Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Absolutely love this watch! Get compliments al...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>14661224</td>\n",
              "      <td>RKH8BNC3L5DLF</td>\n",
              "      <td>B00D3RGO20</td>\n",
              "      <td>484010722</td>\n",
              "      <td>Kenneth Cole New York Women's KC4944 Automatic...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>I love thiswatch it keeps time wonderfully</td>\n",
              "      <td>I love this watch it keeps time wonderfully.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>27324930</td>\n",
              "      <td>R2HLE8WKZSU3NL</td>\n",
              "      <td>B00DKYC7TK</td>\n",
              "      <td>361166390</td>\n",
              "      <td>Ritche 22mm Black Stainless Steel Bracelet Wat...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>Scratches</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>7211452</td>\n",
              "      <td>R31U3UH5AZ42LL</td>\n",
              "      <td>B000EQS1JW</td>\n",
              "      <td>958035625</td>\n",
              "      <td>Citizen Men's BM8180-03E Eco-Drive Stainless S...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>It works well on me. However, I found cheaper ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>12733322</td>\n",
              "      <td>R2SV659OUJ945Y</td>\n",
              "      <td>B00A6GFD7S</td>\n",
              "      <td>765328221</td>\n",
              "      <td>Orient ER27009B Men's Symphony Automatic Stain...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Beautiful face, but cheap sounding links</td>\n",
              "      <td>Beautiful watch face.  The band looks nice all...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  marketplace  ...  review_date\n",
              "0          US  ...   2015-08-31\n",
              "1          US  ...   2015-08-31\n",
              "2          US  ...   2015-08-31\n",
              "3          US  ...   2015-08-31\n",
              "4          US  ...   2015-08-31\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoRSRJzZA7eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove missing value\n",
        "df.review_body.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbKDogiW79a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the first 1000 data as our training data\n",
        "data = df.loc[:1000, 'review_body'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4KGnVeE8UX",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Tokenizing and Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gSwiUBRE8UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use nltk's English stopwords. Load stopwords and stemmer function from NLTK library.\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e50130X8E8Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# tokenization and stemming\n",
        "def tokenization_and_stemming(text):\n",
        "    # exclude stop words and tokenize the document, generate a list of string \n",
        "    tokens = [word.lower() for word in nltk.word_tokenize(text) if word not in stopwords]\n",
        "\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stemming\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwbA6hETE8Uf",
        "colab_type": "code",
        "outputId": "d59e239c-34ab-4ac1-829e-e49f25b6af61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# tokenization and stemming\n",
        "tokenization_and_stemming(data[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['absolut',\n",
              " 'love',\n",
              " 'watch',\n",
              " 'get',\n",
              " 'compliment',\n",
              " 'almost',\n",
              " 'everi',\n",
              " 'time',\n",
              " 'i',\n",
              " 'wear',\n",
              " 'dainti']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtDXMCeME8Uh",
        "colab_type": "text"
      },
      "source": [
        "Use our defined functions to analyze (i.e. tokenize, stem) our reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNtXZ3RlE8Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. do tokenization and stemming for all the documents\n",
        "# 2. also just do tokenization for all the documents\n",
        "# the goal is to create a mapping from stemmed words to original tokenized words for result interpretation.\n",
        "docs_stemmed = []\n",
        "docs_tokenized = []\n",
        "for i in data:\n",
        "    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n",
        "    docs_stemmed.extend(tokenized_and_stemmed_results)\n",
        "    \n",
        "    tokenized_results = tokenization(i)\n",
        "    docs_tokenized.extend(tokenized_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq4ghw8XxXZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping from stemmed words to original words\n",
        "vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QAWdFqL5E8Uo",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XH7R4pE8Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define vectorizer parameters\n",
        "# Here i set minimum document frequency at 0.01 and maximum document frequency at 0.99 and used built-in stop wards. For this project, i used 1-gram only.\n",
        "# The model allow up to 1000 words.\n",
        "tfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000,\n",
        "                                 min_df=0.01, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
        "\n",
        "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
        "      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whu1pCOiE8Uv",
        "colab_type": "text"
      },
      "source": [
        "Save the terms identified by TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdKk6n-E8Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words\n",
        "tf_selected_words = tfidf_model.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWvzNWQFB26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out words\n",
        "tf_selected_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEcwtws5E8U8",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: K-means clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7LJQ5i3IE8U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 5\n",
        "\n",
        "# number of clusters\n",
        "km = KMeans(n_clusters=5)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54meYFcIpNUc",
        "colab_type": "code",
        "outputId": "3d6e3b51-8e2c-4bb8-a807-6d4b132670b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "tfidf_matrix"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1000x245 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 7777 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fboVpRAfE8U-",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Analyze K-means Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGs4aIIME8U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create DataFrame films from all of the input files.\n",
        "product = { 'review': df[:1000].product_title, 'cluster': clusters}\n",
        "frame = pd.DataFrame(product, columns = ['review', 'cluster'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APmEUmm6E8VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame.head(10) # reviews with cluster id assigned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht1SbbOSE8VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Number of reviews included in each cluster:\")\n",
        "frame['cluster'].value_counts().to_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phDfUdh5qL9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean of tf-idf for each feature under each cluster\n",
        "# The purpose there is to find the top 5 importance words for each cluster by searching 6 words with highest mean tf-idf in their cluster\n",
        "km.cluster_centers_ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQg0CF11chKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"<Document clustering result by K-means>\")\n",
        "\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "Cluster_keywords_summary = {}\n",
        "for i in range(num_clusters):\n",
        "    print (\"Cluster \" + str(i) + \" words:\", end='')\n",
        "    Cluster_keywords_summary[i] = []\n",
        "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
        "        Cluster_keywords_summary[i].append(vocab_frame_dict[tf_selected_words[ind]])\n",
        "        print (vocab_frame_dict[tf_selected_words[ind]] + \",\", end='')\n",
        "    print ()\n",
        "    \n",
        "    cluster_reviews = frame[frame.cluster==i].review.tolist()\n",
        "    print (\"Cluster \" + str(i) + \" reviews (\" + str(len(cluster_reviews)) + \" reviews): \")\n",
        "    print (\", \".join(cluster_reviews))\n",
        "    print ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYOZXL53E8VV",
        "colab_type": "text"
      },
      "source": [
        "# Part 5: Topic Modeling - Latent Dirichlet Allocation\n",
        "\n",
        "\n",
        "LDA is a probabilistic model of text used to find topics that describe a corpus. It trades off two conflicting goals:\n",
        "For each document, allocate its words to as few topics as possible.\n",
        "For each topic, assign high probability to as few terms as possible.\n",
        "Trading off these goals finds groups of tightly co-occurring words. A correlated model might work. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPVbFNFE8VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use LDA for clustering\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCACS4Vp9qFB",
        "outputId": "c2a8adf2-e40e-4930-ac81-4e5910cf5558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# LDA requires integer values\n",
        "tfidf_model_lda = CountVectorizer(max_df=0.99, max_features=500,\n",
        "                                 min_df=0.01, stop_words='english',\n",
        "                                 tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
        "\n",
        "tfidf_matrix_lda = tfidf_model_lda.fit_transform(data) #fit the vectorizer to synopses\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix_lda.shape[0]) + \\\n",
        "      \" reviews and \" + str(tfidf_matrix_lda.shape[1]) + \" terms.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In total, there are 1000 reviews and 245 terms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PU80SW391II",
        "outputId": "d02de8e5-591a-4d68-963a-30f2b9d01507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# document topic matrix for tfidf_matrix_lda\n",
        "lda_output = lda.fit_transform(tfidf_matrix_lda)\n",
        "print(lda_output.shape)\n",
        "print(lda_output)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 500)\n",
            "[[0.00025    0.00025    0.00025    ... 0.00025    0.00025    0.00025   ]\n",
            " [0.0005     0.0005     0.0005     ... 0.0005     0.0005     0.0005    ]\n",
            " [0.002      0.002      0.002      ... 0.002      0.002      0.002     ]\n",
            " ...\n",
            " [0.001      0.001      0.001      ... 0.001      0.001      0.001     ]\n",
            " [0.0005     0.0005     0.0005     ... 0.0005     0.0005     0.0005    ]\n",
            " [0.00033333 0.00033333 0.00033333 ... 0.00033333 0.00033333 0.00033333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "hCJs3lBz90vY",
        "outputId": "a0b9c68f-3a60-475e-e930-646209f557a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# topics and words matrix\n",
        "topic_word = lda.components_\n",
        "print(topic_word.shape)\n",
        "print(topic_word)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 245)\n",
            "[[2.00000000e-03 2.00000000e-03 2.00000000e-03 ... 2.00000000e-03\n",
            "  2.00000000e-03 2.00000000e-03]\n",
            " [2.00000000e-03 2.00000000e-03 2.00000000e-03 ... 2.00000000e-03\n",
            "  2.00000000e-03 2.00000000e-03]\n",
            " [2.00000000e-03 2.00000000e-03 2.00000000e-03 ... 2.00000000e-03\n",
            "  2.00000000e-03 2.00000000e-03]\n",
            " ...\n",
            " [2.00000000e-03 2.00200000e+00 2.00000000e-03 ... 2.00000000e-03\n",
            "  2.00000000e-03 1.00200000e+00]\n",
            " [2.00000000e-03 2.00000000e-03 2.00000000e-03 ... 2.00000000e-03\n",
            "  2.00000000e-03 2.00000000e-03]\n",
            " [2.00000000e-03 1.07061859e+00 2.00000000e-03 ... 2.00000000e-03\n",
            "  1.10400242e+00 2.00000000e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCEVfJS5AEgx",
        "colab_type": "code",
        "outputId": "5b18b15c-a7a9-4c1a-c8bf-3201c0d4336b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "# column names\n",
        "topic_names = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
        "\n",
        "# index names\n",
        "doc_names = [\"Doc\" + str(i) for i in range(len(data))]\n",
        "\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
        "\n",
        "# get dominant topic for each document\n",
        "topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['topic'] = topic\n",
        "\n",
        "df_document_topic.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0</th>\n",
              "      <th>Topic1</th>\n",
              "      <th>Topic2</th>\n",
              "      <th>Topic3</th>\n",
              "      <th>Topic4</th>\n",
              "      <th>Topic5</th>\n",
              "      <th>Topic6</th>\n",
              "      <th>Topic7</th>\n",
              "      <th>Topic8</th>\n",
              "      <th>Topic9</th>\n",
              "      <th>Topic10</th>\n",
              "      <th>Topic11</th>\n",
              "      <th>Topic12</th>\n",
              "      <th>Topic13</th>\n",
              "      <th>Topic14</th>\n",
              "      <th>Topic15</th>\n",
              "      <th>Topic16</th>\n",
              "      <th>Topic17</th>\n",
              "      <th>Topic18</th>\n",
              "      <th>Topic19</th>\n",
              "      <th>Topic20</th>\n",
              "      <th>Topic21</th>\n",
              "      <th>Topic22</th>\n",
              "      <th>Topic23</th>\n",
              "      <th>Topic24</th>\n",
              "      <th>Topic25</th>\n",
              "      <th>Topic26</th>\n",
              "      <th>Topic27</th>\n",
              "      <th>Topic28</th>\n",
              "      <th>Topic29</th>\n",
              "      <th>Topic30</th>\n",
              "      <th>Topic31</th>\n",
              "      <th>Topic32</th>\n",
              "      <th>Topic33</th>\n",
              "      <th>Topic34</th>\n",
              "      <th>Topic35</th>\n",
              "      <th>Topic36</th>\n",
              "      <th>Topic37</th>\n",
              "      <th>Topic38</th>\n",
              "      <th>Topic39</th>\n",
              "      <th>...</th>\n",
              "      <th>Topic461</th>\n",
              "      <th>Topic462</th>\n",
              "      <th>Topic463</th>\n",
              "      <th>Topic464</th>\n",
              "      <th>Topic465</th>\n",
              "      <th>Topic466</th>\n",
              "      <th>Topic467</th>\n",
              "      <th>Topic468</th>\n",
              "      <th>Topic469</th>\n",
              "      <th>Topic470</th>\n",
              "      <th>Topic471</th>\n",
              "      <th>Topic472</th>\n",
              "      <th>Topic473</th>\n",
              "      <th>Topic474</th>\n",
              "      <th>Topic475</th>\n",
              "      <th>Topic476</th>\n",
              "      <th>Topic477</th>\n",
              "      <th>Topic478</th>\n",
              "      <th>Topic479</th>\n",
              "      <th>Topic480</th>\n",
              "      <th>Topic481</th>\n",
              "      <th>Topic482</th>\n",
              "      <th>Topic483</th>\n",
              "      <th>Topic484</th>\n",
              "      <th>Topic485</th>\n",
              "      <th>Topic486</th>\n",
              "      <th>Topic487</th>\n",
              "      <th>Topic488</th>\n",
              "      <th>Topic489</th>\n",
              "      <th>Topic490</th>\n",
              "      <th>Topic491</th>\n",
              "      <th>Topic492</th>\n",
              "      <th>Topic493</th>\n",
              "      <th>Topic494</th>\n",
              "      <th>Topic495</th>\n",
              "      <th>Topic496</th>\n",
              "      <th>Topic497</th>\n",
              "      <th>Topic498</th>\n",
              "      <th>Topic499</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Topic0  Topic1  Topic2  Topic3  ...  Topic497  Topic498  Topic499  topic\n",
              "Doc0     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0     28\n",
              "Doc1     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0    450\n",
              "Doc2     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0      0\n",
              "Doc3     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0    238\n",
              "Doc4     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0    355\n",
              "\n",
              "[5 rows x 501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InPLDW7kBSOc",
        "colab_type": "code",
        "outputId": "b2d77cd2-7a2f-4df4-811b-c98c23446e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_document_topic['topic'].value_counts().to_frame()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     topic\n",
              "280     43\n",
              "134     43\n",
              "285     40\n",
              "450     31\n",
              "0       23\n",
              "..     ...\n",
              "336      1\n",
              "152      1\n",
              "154      1\n",
              "335      1\n",
              "499      1\n",
              "\n",
              "[240 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yLe_RFHCz0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# topic word matrix\n",
        "print(lda.components_)\n",
        "# topic-word matrix\n",
        "df_topic_words = pd.DataFrame(lda.components_)\n",
        "\n",
        "# column and index\n",
        "df_topic_words.columns = tfidf_model_lda.get_feature_names()\n",
        "df_topic_words.index = topic_names\n",
        "\n",
        "df_topic_words.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbU9U8V-DFDX",
        "colab_type": "code",
        "outputId": "93f5513d-0777-48a1-91b7-cc6aae299228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "# print top n keywords for each topic\n",
        "def print_topic_words(tfidf_model, lda_model, n_words):\n",
        "    words = np.array(tfidf_model.get_feature_names())\n",
        "    topic_words = []\n",
        "    # for each topic, we have words weight\n",
        "    for topic_words_weights in lda_model.components_:\n",
        "        top_words = topic_words_weights.argsort()[::-1][:n_words]\n",
        "        topic_words.append(words.take(top_words))\n",
        "    return topic_words\n",
        "\n",
        "topic_keywords = print_topic_words(tfidf_model=tfidf_model_lda, lda_model=lda, n_words=15)        \n",
        "\n",
        "df_topic_words = pd.DataFrame(topic_keywords)\n",
        "df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]\n",
        "df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]\n",
        "df_topic_words"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 0</th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Word 3</th>\n",
              "      <th>Word 4</th>\n",
              "      <th>Word 5</th>\n",
              "      <th>Word 6</th>\n",
              "      <th>Word 7</th>\n",
              "      <th>Word 8</th>\n",
              "      <th>Word 9</th>\n",
              "      <th>Word 10</th>\n",
              "      <th>Word 11</th>\n",
              "      <th>Word 12</th>\n",
              "      <th>Word 13</th>\n",
              "      <th>Word 14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic 0</th>\n",
              "      <td>year</td>\n",
              "      <td>gave</td>\n",
              "      <td>fit</td>\n",
              "      <td>finish</td>\n",
              "      <td>fine</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>fair</td>\n",
              "      <td>face</td>\n",
              "      <td>expens</td>\n",
              "      <td>expect</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 1</th>\n",
              "      <td>year</td>\n",
              "      <td>gave</td>\n",
              "      <td>fit</td>\n",
              "      <td>finish</td>\n",
              "      <td>fine</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>fair</td>\n",
              "      <td>face</td>\n",
              "      <td>expens</td>\n",
              "      <td>expect</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 2</th>\n",
              "      <td>valu</td>\n",
              "      <td>great</td>\n",
              "      <td>watch</td>\n",
              "      <td>look</td>\n",
              "      <td>n't</td>\n",
              "      <td>purchas</td>\n",
              "      <td>hope</td>\n",
              "      <td>amaz</td>\n",
              "      <td>better</td>\n",
              "      <td>qualiti</td>\n",
              "      <td>deal</td>\n",
              "      <td>invicta</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 3</th>\n",
              "      <td>disappoint</td>\n",
              "      <td>open</td>\n",
              "      <td>watch</td>\n",
              "      <td>finish</td>\n",
              "      <td>replac</td>\n",
              "      <td>purchas</td>\n",
              "      <td>pin</td>\n",
              "      <td>comfort</td>\n",
              "      <td>somewhat</td>\n",
              "      <td>came</td>\n",
              "      <td>dress</td>\n",
              "      <td>expens</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 4</th>\n",
              "      <td>look</td>\n",
              "      <td>watch</td>\n",
              "      <td>gift</td>\n",
              "      <td>got</td>\n",
              "      <td>featur</td>\n",
              "      <td>big</td>\n",
              "      <td>heavi</td>\n",
              "      <td>husband</td>\n",
              "      <td>nice</td>\n",
              "      <td>button</td>\n",
              "      <td>lot</td>\n",
              "      <td>dress</td>\n",
              "      <td>love</td>\n",
              "      <td>fair</td>\n",
              "      <td>finish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 495</th>\n",
              "      <td>year</td>\n",
              "      <td>gave</td>\n",
              "      <td>fit</td>\n",
              "      <td>finish</td>\n",
              "      <td>fine</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>fair</td>\n",
              "      <td>face</td>\n",
              "      <td>expens</td>\n",
              "      <td>expect</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 496</th>\n",
              "      <td>year</td>\n",
              "      <td>gave</td>\n",
              "      <td>fit</td>\n",
              "      <td>finish</td>\n",
              "      <td>fine</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>fair</td>\n",
              "      <td>face</td>\n",
              "      <td>expens</td>\n",
              "      <td>expect</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 497</th>\n",
              "      <td>watch</td>\n",
              "      <td>'s</td>\n",
              "      <td>easi</td>\n",
              "      <td>year</td>\n",
              "      <td>wear</td>\n",
              "      <td>display</td>\n",
              "      <td>say</td>\n",
              "      <td>comfort</td>\n",
              "      <td>small</td>\n",
              "      <td>use</td>\n",
              "      <td>ve</td>\n",
              "      <td>like</td>\n",
              "      <td>amaz</td>\n",
              "      <td>dial</td>\n",
              "      <td>fit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 498</th>\n",
              "      <td>year</td>\n",
              "      <td>gave</td>\n",
              "      <td>fit</td>\n",
              "      <td>finish</td>\n",
              "      <td>fine</td>\n",
              "      <td>fell</td>\n",
              "      <td>feel</td>\n",
              "      <td>featur</td>\n",
              "      <td>fast</td>\n",
              "      <td>far</td>\n",
              "      <td>fair</td>\n",
              "      <td>face</td>\n",
              "      <td>expens</td>\n",
              "      <td>expect</td>\n",
              "      <td>excel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 499</th>\n",
              "      <td>br</td>\n",
              "      <td>watch</td>\n",
              "      <td>size</td>\n",
              "      <td>band</td>\n",
              "      <td>beauti</td>\n",
              "      <td>n't</td>\n",
              "      <td>featur</td>\n",
              "      <td>good</td>\n",
              "      <td>pin</td>\n",
              "      <td>date</td>\n",
              "      <td>heavi</td>\n",
              "      <td>remov</td>\n",
              "      <td>time</td>\n",
              "      <td>wind</td>\n",
              "      <td>use</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               Word 0 Word 1 Word 2  Word 3  ...  Word 11 Word 12 Word 13 Word 14\n",
              "Topic 0          year   gave    fit  finish  ...     face  expens  expect   excel\n",
              "Topic 1          year   gave    fit  finish  ...     face  expens  expect   excel\n",
              "Topic 2          valu  great  watch    look  ...  invicta    fast     far   excel\n",
              "Topic 3    disappoint   open  watch  finish  ...   expens    fell    feel  featur\n",
              "Topic 4          look  watch   gift     got  ...    dress    love    fair  finish\n",
              "...               ...    ...    ...     ...  ...      ...     ...     ...     ...\n",
              "Topic 495        year   gave    fit  finish  ...     face  expens  expect   excel\n",
              "Topic 496        year   gave    fit  finish  ...     face  expens  expect   excel\n",
              "Topic 497       watch     's   easi    year  ...     like    amaz    dial     fit\n",
              "Topic 498        year   gave    fit  finish  ...     face  expens  expect   excel\n",
              "Topic 499          br  watch   size    band  ...    remov    time    wind     use\n",
              "\n",
              "[500 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}